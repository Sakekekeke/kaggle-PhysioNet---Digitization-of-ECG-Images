{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a504aa-d6a8-4ad6-ad84-cf213700ff12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Initializing Master Dataset...\n",
      "Scanning files in C:\\Users\\fujiw\\OneDrive\\デスクトップ\\ECG_ResNet\\stage2...\n",
      "Found 8793 valid files. Loading ALL into RAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|████████████| 8793/8793 [02:47<00:00, 52.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 8793 samples.\n",
      "\n",
      "========================================\n",
      " Starting 5-Fold CV (GroupKFold)\n",
      "========================================\n",
      "\n",
      ">>> Fold 1 / 5\n",
      "  [Fold 1 Epoch 1] Train: 0.063564 | Val: 0.017515 | LR: 1.0e-03\n",
      "  [Fold 1 Epoch 2] Train: 0.020725 | Val: 0.014785 | LR: 1.0e-03\n",
      "  [Fold 1 Epoch 3] Train: 0.019278 | Val: 0.013410 | LR: 1.0e-03\n",
      "  [Fold 1 Epoch 5] Train: 0.017219 | Val: 0.012787 | LR: 1.0e-03\n",
      "  [Fold 1 Epoch 6] Train: 0.016656 | Val: 0.012085 | LR: 1.0e-03\n",
      "  [Fold 1 Epoch 7] Train: 0.016650 | Val: 0.011895 | LR: 1.0e-03\n",
      "  [Fold 1 Epoch 11] Train: 0.015642 | Val: 0.012678 | LR: 1.0e-03\n",
      "  [Fold 1 Epoch 16] Train: 0.012281 | Val: 0.009228 | LR: 1.0e-04\n",
      "  [Fold 1 Epoch 17] Train: 0.011528 | Val: 0.009085 | LR: 1.0e-04\n",
      "  [Fold 1 Epoch 19] Train: 0.010547 | Val: 0.009013 | LR: 1.0e-04\n",
      "  [Fold 1 Epoch 21] Train: 0.010193 | Val: 0.009067 | LR: 1.0e-04\n",
      "  [Fold 1 Epoch 26] Train: 0.009436 | Val: 0.009392 | LR: 1.0e-04\n",
      "  [Fold 1 Epoch 28] Train: 0.008727 | Val: 0.008951 | LR: 1.0e-05\n",
      "  [Fold 1 Epoch 31] Train: 0.008539 | Val: 0.009094 | LR: 1.0e-05\n",
      "  [Fold 1 Epoch 33] Train: 0.008529 | Val: 0.008939 | LR: 1.0e-05\n",
      "  [Fold 1 Epoch 36] Train: 0.008405 | Val: 0.009145 | LR: 1.0e-05\n",
      "  [Fold 1 Epoch 41] Train: 0.008259 | Val: 0.009145 | LR: 1.0e-06\n",
      "  [Fold 1 Epoch 46] Train: 0.008144 | Val: 0.009210 | LR: 1.0e-06\n",
      "  [Fold 1 Epoch 51] Train: 0.008117 | Val: 0.009229 | LR: 1.0e-07\n",
      "  Early stopping at epoch 53. Best Val Loss: 0.008939\n",
      "Fold 1 Finished. Best Loss: 0.008939\n",
      "\n",
      ">>> Fold 2 / 5\n",
      "  [Fold 2 Epoch 1] Train: 0.064731 | Val: 0.022314 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 2] Train: 0.019918 | Val: 0.020198 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 4] Train: 0.016912 | Val: 0.019262 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 5] Train: 0.016236 | Val: 0.018230 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 6] Train: 0.015925 | Val: 0.020383 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 7] Train: 0.015526 | Val: 0.016960 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 11] Train: 0.014919 | Val: 0.015969 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 12] Train: 0.014637 | Val: 0.015096 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 16] Train: 0.014098 | Val: 0.017359 | LR: 1.0e-03\n",
      "  [Fold 2 Epoch 21] Train: 0.011335 | Val: 0.013553 | LR: 1.0e-04\n",
      "  [Fold 2 Epoch 26] Train: 0.009340 | Val: 0.014712 | LR: 1.0e-04\n",
      "  [Fold 2 Epoch 31] Train: 0.008578 | Val: 0.014767 | LR: 1.0e-05\n",
      "  [Fold 2 Epoch 36] Train: 0.008508 | Val: 0.014849 | LR: 1.0e-05\n",
      "  [Fold 2 Epoch 41] Train: 0.008357 | Val: 0.014029 | LR: 1.0e-06\n",
      "  Early stopping at epoch 41. Best Val Loss: 0.013553\n",
      "Fold 2 Finished. Best Loss: 0.013553\n",
      "\n",
      ">>> Fold 3 / 5\n",
      "  [Fold 3 Epoch 1] Train: 0.065086 | Val: 0.020085 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 2] Train: 0.020988 | Val: 0.015872 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 4] Train: 0.018016 | Val: 0.014151 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 5] Train: 0.017463 | Val: 0.013073 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 6] Train: 0.017166 | Val: 0.013261 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 8] Train: 0.016223 | Val: 0.012906 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 9] Train: 0.015970 | Val: 0.012319 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 10] Train: 0.015494 | Val: 0.010837 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 11] Train: 0.015302 | Val: 0.013956 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 16] Train: 0.014963 | Val: 0.015412 | LR: 1.0e-03\n",
      "  [Fold 3 Epoch 19] Train: 0.011350 | Val: 0.008649 | LR: 1.0e-04\n",
      "  [Fold 3 Epoch 21] Train: 0.010607 | Val: 0.008794 | LR: 1.0e-04\n",
      "  [Fold 3 Epoch 26] Train: 0.010060 | Val: 0.009267 | LR: 1.0e-04\n",
      "  [Fold 3 Epoch 29] Train: 0.009492 | Val: 0.008602 | LR: 1.0e-05\n",
      "  [Fold 3 Epoch 30] Train: 0.009471 | Val: 0.008523 | LR: 1.0e-05\n",
      "  [Fold 3 Epoch 31] Train: 0.009436 | Val: 0.008619 | LR: 1.0e-05\n",
      "  [Fold 3 Epoch 34] Train: 0.009361 | Val: 0.008504 | LR: 1.0e-05\n",
      "  [Fold 3 Epoch 36] Train: 0.009334 | Val: 0.008609 | LR: 1.0e-05\n",
      "  [Fold 3 Epoch 41] Train: 0.009233 | Val: 0.008678 | LR: 1.0e-05\n",
      "  [Fold 3 Epoch 46] Train: 0.009085 | Val: 0.008622 | LR: 1.0e-06\n",
      "  [Fold 3 Epoch 51] Train: 0.009095 | Val: 0.008523 | LR: 1.0e-07\n",
      "  Early stopping at epoch 54. Best Val Loss: 0.008504\n",
      "Fold 3 Finished. Best Loss: 0.008504\n",
      "\n",
      ">>> Fold 4 / 5\n",
      "  [Fold 4 Epoch 1] Train: 0.062318 | Val: 0.034166 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 3] Train: 0.015099 | Val: 0.028990 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 4] Train: 0.013919 | Val: 0.028188 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 6] Train: 0.013071 | Val: 0.027920 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 8] Train: 0.012331 | Val: 0.027801 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 9] Train: 0.012183 | Val: 0.027281 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 10] Train: 0.012134 | Val: 0.025974 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 11] Train: 0.011666 | Val: 0.027835 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 14] Train: 0.011109 | Val: 0.025045 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 16] Train: 0.010924 | Val: 0.025815 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 21] Train: 0.010281 | Val: 0.029635 | LR: 1.0e-03\n",
      "  [Fold 4 Epoch 23] Train: 0.007727 | Val: 0.023310 | LR: 1.0e-04\n",
      "  [Fold 4 Epoch 24] Train: 0.007412 | Val: 0.023161 | LR: 1.0e-04\n",
      "  [Fold 4 Epoch 25] Train: 0.007264 | Val: 0.023158 | LR: 1.0e-04\n",
      "  [Fold 4 Epoch 26] Train: 0.007197 | Val: 0.023134 | LR: 1.0e-04\n",
      "  [Fold 4 Epoch 31] Train: 0.006834 | Val: 0.023484 | LR: 1.0e-04\n",
      "  [Fold 4 Epoch 35] Train: 0.006209 | Val: 0.022946 | LR: 1.0e-05\n",
      "  [Fold 4 Epoch 36] Train: 0.006142 | Val: 0.023009 | LR: 1.0e-05\n",
      "  [Fold 4 Epoch 41] Train: 0.006066 | Val: 0.022981 | LR: 1.0e-05\n",
      "  [Fold 4 Epoch 46] Train: 0.006020 | Val: 0.022985 | LR: 1.0e-06\n",
      "  [Fold 4 Epoch 51] Train: 0.006007 | Val: 0.022992 | LR: 1.0e-07\n",
      "  Early stopping at epoch 55. Best Val Loss: 0.022946\n",
      "Fold 4 Finished. Best Loss: 0.022946\n",
      "\n",
      ">>> Fold 5 / 5\n",
      "  [Fold 5 Epoch 1] Train: 0.065494 | Val: 0.025516 | LR: 1.0e-03\n",
      "  [Fold 5 Epoch 2] Train: 0.019944 | Val: 0.020508 | LR: 1.0e-03\n",
      "  [Fold 5 Epoch 6] Train: 0.015901 | Val: 0.021628 | LR: 1.0e-03\n",
      "  [Fold 5 Epoch 7] Train: 0.015745 | Val: 0.020128 | LR: 1.0e-03\n",
      "  [Fold 5 Epoch 11] Train: 0.015221 | Val: 0.017259 | LR: 1.0e-03\n",
      "  [Fold 5 Epoch 13] Train: 0.014489 | Val: 0.017093 | LR: 1.0e-03\n",
      "  [Fold 5 Epoch 16] Train: 0.014156 | Val: 0.018327 | LR: 1.0e-03\n",
      "  [Fold 5 Epoch 21] Train: 0.012492 | Val: 0.018066 | LR: 1.0e-04\n",
      "  [Fold 5 Epoch 22] Train: 0.010402 | Val: 0.014873 | LR: 1.0e-04\n",
      "  [Fold 5 Epoch 23] Train: 0.010047 | Val: 0.014681 | LR: 1.0e-04\n",
      "  [Fold 5 Epoch 26] Train: 0.009816 | Val: 0.015739 | LR: 1.0e-04\n",
      "  [Fold 5 Epoch 31] Train: 0.009503 | Val: 0.015871 | LR: 1.0e-05\n",
      "  [Fold 5 Epoch 36] Train: 0.009026 | Val: 0.015951 | LR: 1.0e-05\n",
      "  [Fold 5 Epoch 41] Train: 0.008939 | Val: 0.015872 | LR: 1.0e-06\n",
      "  Early stopping at epoch 43. Best Val Loss: 0.014681\n",
      "Fold 5 Finished. Best Loss: 0.014681\n",
      "\n",
      "========================================\n",
      " CV FINISHED \n",
      "========================================\n",
      "Fold 1: 0.008939\n",
      "Fold 2: 0.013553\n",
      "Fold 3: 0.008504\n",
      "Fold 4: 0.022946\n",
      "Fold 5: 0.014681\n",
      "Average: 0.013725\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from scipy.signal import resample\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold \n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0. 設定エリア\n",
    "# =========================================================\n",
    "BASE_DIR = r\"C:\\Users\\fujiw\\OneDrive\\デスクトップ\\ECG_ResNet\"\n",
    "\n",
    "STAGE2_DIR = os.path.join(BASE_DIR, \"stage2\")\n",
    "CSV_DIR    = os.path.join(BASE_DIR, \"train_csvs\")\n",
    "TRAIN_META = os.path.join(BASE_DIR, \"train.csv\")\n",
    "SAVE_DIR   = os.path.join(BASE_DIR, \"seeed30\")\n",
    "\n",
    "# ハイパーパラメータ\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "LR = 1e-3\n",
    "PATIENCE = 20    # 停滞許容回数\n",
    "SEED = 30\n",
    "N_FOLDS = 5      # 5分割\n",
    "\n",
    "# ★ブレーキを少し緩める（学習不足解消のため）\n",
    "WEIGHT_DECAY = 1e-4 \n",
    "\n",
    "# =========================================================\n",
    "# 1. Utils\n",
    "# =========================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# =========================================================\n",
    "# 2. Dataset Class\n",
    "# =========================================================\n",
    "class ECGDatasetRam(Dataset):\n",
    "    def __init__(self, df, npy_dir, csv_dir, target_len=5000):\n",
    "        self.target_len = target_len\n",
    "        self.samples = [] \n",
    "        self.sample_ids = []  # ★追加: IDを記録するリスト\n",
    "        if not os.path.exists(npy_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {npy_dir}\")\n",
    "            \n",
    "        target_ids = set(df['id'].astype(str).tolist())\n",
    "        file_list = []\n",
    "        all_files = glob.glob(os.path.join(npy_dir, \"*.npy\"))\n",
    "        \n",
    "        print(f\"Scanning files in {npy_dir}...\")\n",
    "        for fpath in all_files:\n",
    "            fname = os.path.basename(fpath)\n",
    "            file_id = fname.split('-')[0]\n",
    "            if file_id in target_ids:\n",
    "                file_list.append((fpath, file_id))\n",
    "        \n",
    "        print(f\"Found {len(file_list)} valid files. Loading ALL into RAM...\")\n",
    "\n",
    "        for fpath, sample_id in tqdm(file_list, desc=\"Loading Data\"):\n",
    "            processed = self.process_one_file(fpath, sample_id, csv_dir)\n",
    "            if processed is not None:\n",
    "                self.samples.append(processed)\n",
    "                self.sample_ids.append(sample_id) # ★追加: 成功したデータのIDだけ記録\n",
    "                \n",
    "        print(f\"Successfully loaded {len(self.samples)} samples.\")\n",
    "\n",
    "    def process_one_file(self, npy_path, sample_id, csv_dir):\n",
    "        try:\n",
    "            # Input\n",
    "            data = np.load(npy_path)\n",
    "            data = np.nan_to_num(data, nan=0.0)\n",
    "            original_len = data.shape[1]\n",
    "            if data.shape[0] != 13: return None\n",
    "\n",
    "            reconstructed = np.zeros((12, original_len), dtype=np.float32)\n",
    "            for i in range(4):\n",
    "                sig_row = data[i]\n",
    "                id_row = data[9+i]\n",
    "                unique_ids = np.unique(id_row)\n",
    "                for uid in unique_ids:\n",
    "                    if 0 <= uid <= 11:\n",
    "                        mask_ch = (id_row == uid)\n",
    "                        reconstructed[int(uid), mask_ch] = sig_row[mask_ch]\n",
    "            \n",
    "            # Target\n",
    "            csv_path = os.path.join(csv_dir, f\"{sample_id}.csv\")\n",
    "            if not os.path.exists(csv_path): return None\n",
    "\n",
    "            target_df = pd.read_csv(csv_path)\n",
    "            target_vals = target_df.values.T \n",
    "            mask_data = (~np.isnan(target_vals)).astype(np.float32)\n",
    "            target_data = np.nan_to_num(target_vals, nan=0.0)\n",
    "            \n",
    "            if reconstructed.shape[1] != self.target_len:\n",
    "                input_final = resample(reconstructed, self.target_len, axis=1)\n",
    "            else:\n",
    "                input_final = reconstructed\n",
    "                \n",
    "            if target_data.shape[1] != self.target_len:\n",
    "                target_final = resample(target_data, self.target_len, axis=1)\n",
    "                mask_final = resample(mask_data, self.target_len, axis=1)\n",
    "            else:\n",
    "                target_final = target_data\n",
    "                mask_final = mask_data\n",
    "\n",
    "            # Standardization\n",
    "            mean = np.mean(input_final, axis=1, keepdims=True)\n",
    "            std = np.std(input_final, axis=1, keepdims=True) + 1e-6\n",
    "            input_final = (input_final - mean) / std\n",
    "            \n",
    "            return (np.nan_to_num(input_final).astype(np.float32), \n",
    "                    np.nan_to_num(target_final).astype(np.float32), \n",
    "                    (mask_final > 0.5).astype(np.float32), \n",
    "                    original_len)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        in_arr, tgt_arr, msk_arr, length = self.samples[idx]\n",
    "        return (torch.from_numpy(in_arr), torch.from_numpy(tgt_arr), \n",
    "                torch.from_numpy(msk_arr), torch.tensor(length, dtype=torch.long))\n",
    "\n",
    "# =========================================================\n",
    "# 3. Model (Dropout率を調整済)\n",
    "# =========================================================\n",
    "class ResNet1d_UNet_Large(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: Dropoutを削除\n",
    "        self.enc1 = nn.Sequential(nn.Conv1d(12, 128, 7, 2, 3), nn.BatchNorm1d(128), nn.ReLU()) # , nn.Dropout(0.1) 削除\n",
    "        self.enc2 = nn.Sequential(nn.Conv1d(128, 256, 3, 2, 1), nn.BatchNorm1d(256), nn.ReLU()) # , nn.Dropout(0.2) 削除\n",
    "        self.enc3 = nn.Sequential(nn.Conv1d(256, 512, 3, 2, 1), nn.BatchNorm1d(512), nn.ReLU()) # , nn.Dropout(0.2) 削除\n",
    "        self.enc4 = nn.Sequential(nn.Conv1d(512, 1024, 3, 2, 1), nn.BatchNorm1d(1024), nn.ReLU())\n",
    "        \n",
    "        # Bottleneck: Dropoutを削除\n",
    "        # self.dropout = nn.Dropout(0.3) 削除\n",
    "\n",
    "        # Decoder: Dropoutを削除\n",
    "        self.dec4 = nn.Sequential(nn.Conv1d(1024 + 512, 512, 3, 1, 1), nn.BatchNorm1d(512), nn.ReLU()) # , nn.Dropout(0.1) 削除\n",
    "        self.dec3 = nn.Sequential(nn.Conv1d(512 + 256, 256, 3, 1, 1), nn.BatchNorm1d(256), nn.ReLU()) # , nn.Dropout(0.1) 削除\n",
    "        self.dec2 = nn.Sequential(nn.Conv1d(256 + 128, 128, 3, 1, 1), nn.BatchNorm1d(128), nn.ReLU())\n",
    "        self.dec1 = nn.Sequential(nn.Conv1d(128, 64, 3, 1, 1), nn.BatchNorm1d(64), nn.ReLU())\n",
    "        \n",
    "        self.final = nn.Conv1d(64, 12, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        # e4 = self.dropout(e4) # 削除\n",
    "        # ... (以下同じ)\n",
    "        d4 = torch.cat([torch.nn.functional.interpolate(e4, size=e3.shape[2]), e3], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = torch.cat([torch.nn.functional.interpolate(d4, size=e2.shape[2]), e2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = torch.cat([torch.nn.functional.interpolate(d3, size=e1.shape[2]), e1], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = torch.nn.functional.interpolate(d2, size=x.shape[2])\n",
    "        d1 = self.dec1(d1)\n",
    "        out = self.final(d1)\n",
    "        return x + out\n",
    "\n",
    "# =========================================================\n",
    "# 4. Main Training Loop (5-Fold CV)\n",
    "# =========================================================\n",
    "# =========================================================\n",
    "# 4. Main Training Loop (5-Fold CV)\n",
    "# =========================================================\n",
    "def run_kfold_training():\n",
    "    seed_everything(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "    \n",
    "    if not os.path.exists(SAVE_DIR): os.makedirs(SAVE_DIR)\n",
    "    \n",
    "    # 全データをロード\n",
    "    print(\"Initializing Master Dataset...\")\n",
    "    if not os.path.exists(TRAIN_META): return\n",
    "    full_df = pd.read_csv(TRAIN_META)\n",
    "    \n",
    "    full_ds = ECGDatasetRam(full_df, STAGE2_DIR, CSV_DIR)\n",
    "    \n",
    "    # ▼▼▼ 変更箇所ここから ▼▼▼\n",
    "    \n",
    "    # ★変更前: ランダムシャッフル (KFold)\n",
    "    # kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # ★変更後: ID考慮の分割 (GroupKFold)\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    \n",
    "    # ★グループ（患者ID）のリストを取得\n",
    "    groups = full_ds.sample_ids \n",
    "    \n",
    "    # ▲▲▲ 変更箇所ここまで ▲▲▲\n",
    "\n",
    "    fold_scores = []\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\" Starting {N_FOLDS}-Fold CV (GroupKFold)\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # ★変更: splitに groups=groups を渡す\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(range(len(full_ds)), groups=groups)):\n",
    "        print(f\"\\n>>> Fold {fold+1} / {N_FOLDS}\")\n",
    "        \n",
    "        train_ds = Subset(full_ds, train_idx)\n",
    "        val_ds = Subset(full_ds, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        model = ResNet1d_UNet_Large().to(device)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7)\n",
    "        \n",
    "        use_amp = torch.cuda.is_available()\n",
    "        scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "        criterion_raw = nn.MSELoss(reduction='none')\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for inputs, targets, masks, _ in train_loader:\n",
    "                inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = (criterion_raw(outputs, targets) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = (criterion_raw(outputs, targets) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # Valid\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets, masks, _ in val_loader:\n",
    "                    inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            outputs = model(inputs)\n",
    "                            loss = (criterion_raw(outputs, targets) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = (criterion_raw(outputs, targets) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                    val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if epoch % 5 == 0 or avg_val_loss < best_loss:\n",
    "                 current_lr = optimizer.param_groups[0]['lr']\n",
    "                 print(f\"  [Fold {fold+1} Epoch {epoch+1}] Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | LR: {current_lr:.1e}\")\n",
    "\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                save_path = os.path.join(SAVE_DIR, f\"best_model_fold{fold}.pth\")\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= PATIENCE:\n",
    "                    print(f\"  Early stopping at epoch {epoch+1}. Best Val Loss: {best_loss:.6f}\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"Fold {fold+1} Finished. Best Loss: {best_loss:.6f}\")\n",
    "        fold_scores.append(best_loss)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\" CV FINISHED \")\n",
    "    print(\"=\"*40)\n",
    "    for i, score in enumerate(fold_scores):\n",
    "        print(f\"Fold {i+1}: {score:.6f}\")\n",
    "    print(f\"Average: {np.mean(fold_scores):.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_kfold_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44922cb0-dd93-437b-b039-166b0dbd7e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44b216-db4d-4781-8f54-b6c3bd56eff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
