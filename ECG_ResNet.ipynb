{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a504aa-d6a8-4ad6-ad84-cf213700ff12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Initializing Master Dataset...\n",
      "Scanning files in C:\\Users\\fujiw\\OneDrive\\デスクトップ\\ECG_ResNet\\stage2...\n",
      "Found 8793 valid files. Loading ALL into RAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data:  49%|█████▊      | 4293/8793 [01:03<01:38, 45.72it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from scipy.signal import resample\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold \n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0. 設定エリア\n",
    "# =========================================================\n",
    "BASE_DIR = r\"C:\\Users\\fujiw\\OneDrive\\デスクトップ\\ECG_ResNet\"\n",
    "\n",
    "STAGE2_DIR = os.path.join(BASE_DIR, \"stage2\")\n",
    "CSV_DIR    = os.path.join(BASE_DIR, \"train_csvs\")\n",
    "TRAIN_META = os.path.join(BASE_DIR, \"train.csv\")\n",
    "SAVE_DIR   = os.path.join(BASE_DIR, \"seeed30\")\n",
    "\n",
    "# ハイパーパラメータ\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "LR = 1e-3\n",
    "PATIENCE = 20    # 停滞許容回数\n",
    "SEED = 30\n",
    "N_FOLDS = 5      # 5分割\n",
    "\n",
    "# ★ブレーキを少し緩める（学習不足解消のため）\n",
    "WEIGHT_DECAY = 1e-4 \n",
    "\n",
    "# =========================================================\n",
    "# 1. Utils\n",
    "# =========================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# =========================================================\n",
    "# 2. Dataset Class\n",
    "# =========================================================\n",
    "class ECGDatasetRam(Dataset):\n",
    "    def __init__(self, df, npy_dir, csv_dir, target_len=5000):\n",
    "        self.target_len = target_len\n",
    "        self.samples = [] \n",
    "        self.sample_ids = []  # ★追加: IDを記録するリスト\n",
    "        if not os.path.exists(npy_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {npy_dir}\")\n",
    "            \n",
    "        target_ids = set(df['id'].astype(str).tolist())\n",
    "        file_list = []\n",
    "        all_files = glob.glob(os.path.join(npy_dir, \"*.npy\"))\n",
    "        \n",
    "        print(f\"Scanning files in {npy_dir}...\")\n",
    "        for fpath in all_files:\n",
    "            fname = os.path.basename(fpath)\n",
    "            file_id = fname.split('-')[0]\n",
    "            if file_id in target_ids:\n",
    "                file_list.append((fpath, file_id))\n",
    "        \n",
    "        print(f\"Found {len(file_list)} valid files. Loading ALL into RAM...\")\n",
    "\n",
    "        for fpath, sample_id in tqdm(file_list, desc=\"Loading Data\"):\n",
    "            processed = self.process_one_file(fpath, sample_id, csv_dir)\n",
    "            if processed is not None:\n",
    "                self.samples.append(processed)\n",
    "                self.sample_ids.append(sample_id) # ★追加: 成功したデータのIDだけ記録\n",
    "                \n",
    "        print(f\"Successfully loaded {len(self.samples)} samples.\")\n",
    "    \n",
    "    def process_one_file(self, npy_path, sample_id, csv_dir):\n",
    "        try:\n",
    "            # ---------------------------------------------------------\n",
    "            # 1. Input Load (新しい処理: 信号12ch + 確信度12ch = 24ch)\n",
    "            # ---------------------------------------------------------\n",
    "            data = np.load(npy_path)\n",
    "            data = np.nan_to_num(data, nan=0.0)\n",
    "            original_len = data.shape[1]\n",
    "            if data.shape[0] != 13: return None\n",
    "\n",
    "            # 信号用と確信度用の器を用意\n",
    "            reconstructed_sig = np.zeros((12, original_len), dtype=np.float32)\n",
    "            reconstructed_conf = np.zeros((12, original_len), dtype=np.float32)\n",
    "\n",
    "            for i in range(4):\n",
    "                sig_row = data[i]       # 信号\n",
    "                conf_row = data[4+i]    # 確信度\n",
    "                id_row = data[9+i]      # ID\n",
    "                \n",
    "                unique_ids = np.unique(id_row)\n",
    "                for uid in unique_ids:\n",
    "                    if 0 <= uid <= 11:\n",
    "                        mask_ch = (id_row == uid)\n",
    "                        reconstructed_sig[int(uid), mask_ch] = sig_row[mask_ch]\n",
    "                        reconstructed_conf[int(uid), mask_ch] = conf_row[mask_ch]\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # 2. Target Load (以前の処理を復元: CSV読み込み)\n",
    "            # ---------------------------------------------------------\n",
    "            csv_path = os.path.join(csv_dir, f\"{sample_id}.csv\")\n",
    "            if not os.path.exists(csv_path): return None\n",
    "\n",
    "            target_df = pd.read_csv(csv_path)\n",
    "            target_vals = target_df.values.T \n",
    "            mask_data = (~np.isnan(target_vals)).astype(np.float32)\n",
    "            target_data = np.nan_to_num(target_vals, nan=0.0)\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # 3. Resample & Preprocessing (両方の処理を適用)\n",
    "            # ---------------------------------------------------------\n",
    "            \n",
    "            # (A) Input Resample\n",
    "            if reconstructed_sig.shape[1] != self.target_len:\n",
    "                input_sig_final = resample(reconstructed_sig, self.target_len, axis=1)\n",
    "                input_conf_final = resample(reconstructed_conf, self.target_len, axis=1)\n",
    "            else:\n",
    "                input_sig_final = reconstructed_sig\n",
    "                input_conf_final = reconstructed_conf\n",
    "\n",
    "            # (B) Target Resample\n",
    "            if target_data.shape[1] != self.target_len:\n",
    "                target_final = resample(target_data, self.target_len, axis=1)\n",
    "                mask_final = resample(mask_data, self.target_len, axis=1)\n",
    "            else:\n",
    "                target_final = target_data\n",
    "                mask_final = mask_data\n",
    "            \n",
    "            # (D) Concatenate (12ch + 12ch = 24ch)\n",
    "            input_final = np.concatenate([input_sig_final, input_conf_final], axis=0)\n",
    "\n",
    "            return (np.nan_to_num(input_final).astype(np.float32), \n",
    "                    np.nan_to_num(target_final).astype(np.float32), \n",
    "                    (mask_final > 0.5).astype(np.float32), \n",
    "                    original_len)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        in_arr, tgt_arr, msk_arr, length = self.samples[idx]\n",
    "        return (torch.from_numpy(in_arr), torch.from_numpy(tgt_arr), \n",
    "                torch.from_numpy(msk_arr), torch.tensor(length, dtype=torch.long))\n",
    "\n",
    "# =========================================================\n",
    "# 3. Model (修正版)\n",
    "# =========================================================\n",
    "class ResNet1d_UNet_Large(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(nn.Conv1d(24, 128, 7, 2, 3), nn.BatchNorm1d(128), nn.ReLU()) \n",
    "        self.enc2 = nn.Sequential(nn.Conv1d(128, 256, 3, 2, 1), nn.BatchNorm1d(256), nn.ReLU()) \n",
    "        self.enc3 = nn.Sequential(nn.Conv1d(256, 512, 3, 2, 1), nn.BatchNorm1d(512), nn.ReLU()) \n",
    "        self.enc4 = nn.Sequential(nn.Conv1d(512, 1024, 3, 2, 1), nn.BatchNorm1d(1024), nn.ReLU())\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec4 = nn.Sequential(nn.Conv1d(1024 + 512, 512, 3, 1, 1), nn.BatchNorm1d(512), nn.ReLU()) \n",
    "        self.dec3 = nn.Sequential(nn.Conv1d(512 + 256, 256, 3, 1, 1), nn.BatchNorm1d(256), nn.ReLU()) \n",
    "        self.dec2 = nn.Sequential(nn.Conv1d(256 + 128, 128, 3, 1, 1), nn.BatchNorm1d(128), nn.ReLU())\n",
    "        self.dec1 = nn.Sequential(nn.Conv1d(128, 64, 3, 1, 1), nn.BatchNorm1d(64), nn.ReLU())\n",
    "        \n",
    "        self.final = nn.Conv1d(64, 12, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        \n",
    "        d4 = torch.cat([torch.nn.functional.interpolate(e4, size=e3.shape[2]), e3], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        \n",
    "        d3 = torch.cat([torch.nn.functional.interpolate(d4, size=e2.shape[2]), e2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = torch.cat([torch.nn.functional.interpolate(d3, size=e1.shape[2]), e1], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = torch.nn.functional.interpolate(d2, size=x.shape[2])\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        out = self.final(d1)\n",
    "        \n",
    "        # ★修正箇所: 入力x(24ch)のうち、信号部分(先頭12ch)だけを取り出して足す\n",
    "        input_signal = x[:, :12, :] \n",
    "        \n",
    "        return input_signal + out\n",
    "\n",
    "# =========================================================\n",
    "# 4. Main Training Loop (5-Fold CV)\n",
    "# =========================================================\n",
    "def run_kfold_training():\n",
    "    seed_everything(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "    \n",
    "    if not os.path.exists(SAVE_DIR): os.makedirs(SAVE_DIR)\n",
    "    \n",
    "    # 全データをロード\n",
    "    print(\"Initializing Master Dataset...\")\n",
    "    if not os.path.exists(TRAIN_META): return\n",
    "    full_df = pd.read_csv(TRAIN_META)\n",
    "    \n",
    "    full_ds = ECGDatasetRam(full_df, STAGE2_DIR, CSV_DIR)\n",
    "    \n",
    "    # ▼▼▼ 変更箇所ここから ▼▼▼\n",
    "    \n",
    "    # ★変更前: ランダムシャッフル (KFold)\n",
    "    # kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # ★変更後: ID考慮の分割 (GroupKFold)\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    \n",
    "    # ★グループ（患者ID）のリストを取得\n",
    "    groups = full_ds.sample_ids \n",
    "    \n",
    "    # ▲▲▲ 変更箇所ここまで ▲▲▲\n",
    "\n",
    "    fold_scores = []\n",
    "\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\" Starting {N_FOLDS}-Fold CV (GroupKFold)\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # ★変更: splitに groups=groups を渡す\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(range(len(full_ds)), groups=groups)):\n",
    "        print(f\"\\n>>> Fold {fold+1} / {N_FOLDS}\")\n",
    "        \n",
    "        train_ds = Subset(full_ds, train_idx)\n",
    "        val_ds = Subset(full_ds, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        model = ResNet1d_UNet_Large().to(device)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7)\n",
    "        \n",
    "        use_amp = torch.cuda.is_available()\n",
    "        scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "        criterion_raw = nn.MSELoss(reduction='none')\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for inputs, targets, masks, _ in train_loader:\n",
    "                inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = (criterion_raw(outputs, targets) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = (criterion_raw(outputs, targets) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # Valid\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets, masks, _ in val_loader:\n",
    "                    inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast('cuda'):\n",
    "                            outputs = model(inputs)\n",
    "                            loss = (criterion_raw(outputs, targets) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = (criterion_raw(outputs, targets) * masks).sum() / (masks.sum() + 1e-8)\n",
    "                    val_loss += loss.item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            if epoch % 5 == 0 or avg_val_loss < best_loss:\n",
    "                 current_lr = optimizer.param_groups[0]['lr']\n",
    "                 print(f\"  [Fold {fold+1} Epoch {epoch+1}] Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | LR: {current_lr:.1e}\")\n",
    "\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                save_path = os.path.join(SAVE_DIR, f\"best_model_fold{fold}.pth\")\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= PATIENCE:\n",
    "                    print(f\"  Early stopping at epoch {epoch+1}. Best Val Loss: {best_loss:.6f}\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"Fold {fold+1} Finished. Best Loss: {best_loss:.6f}\")\n",
    "        fold_scores.append(best_loss)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\" CV FINISHED \")\n",
    "    print(\"=\"*40)\n",
    "    for i, score in enumerate(fold_scores):\n",
    "        print(f\"Fold {i+1}: {score:.6f}\")\n",
    "    print(f\"Average: {np.mean(fold_scores):.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_kfold_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44922cb0-dd93-437b-b039-166b0dbd7e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44b216-db4d-4781-8f54-b6c3bd56eff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
