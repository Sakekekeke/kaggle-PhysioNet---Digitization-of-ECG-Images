{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a504aa-d6a8-4ad6-ad84-cf213700ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.signal import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# 0. 設定エリア (ここだけ自分のPC環境に合わせて書き換えてください)\n",
    "# =========================================================\n",
    "# 例: データの親フォルダ\n",
    "BASE_DIR = r\"C:\\Users\\fujiw\\OneDrive\\デスクトップ\\ECG_ResNet\"\n",
    "\n",
    "# 各フォルダの場所 (BASE_DIRからの相対パス、または絶対パスで指定)\n",
    "# ※ フォルダ名はあなたの実際の環境に合わせて変更してください\n",
    "STAGE2_DIR = os.path.join(BASE_DIR, \"stage2\")       # .npyファイルが入っているフォルダ\n",
    "CSV_DIR    = os.path.join(BASE_DIR, \"train_csvs\")   # 正解データ(.csv)が入っているフォルダ\n",
    "TRAIN_META = os.path.join(BASE_DIR, \"train.csv\")    # train.csv のパス\n",
    "SAVE_DIR   = os.path.join(BASE_DIR, \"models\")       # モデル(.pth)の保存先\n",
    "\n",
    "# PCのスペックに合わせて変更\n",
    "BATCH_SIZE = 64  # GPUメモリ不足になる場合は 32 に下げてください\n",
    "EPOCHS = 200\n",
    "PATIENCE = 20\n",
    "LR = 1e-3\n",
    "\n",
    "# =========================================================\n",
    "# 1. Dataset Class (処理ロジック変更なし)\n",
    "# =========================================================\n",
    "class ECGDatasetRam(Dataset):\n",
    "    def __init__(self, df, npy_dir, csv_dir, target_len=5000):\n",
    "        self.target_len = target_len\n",
    "        self.samples = [] \n",
    "        \n",
    "        # パス存在チェック\n",
    "        if not os.path.exists(npy_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {npy_dir}\")\n",
    "            \n",
    "        target_ids = set(df['id'].astype(str).tolist())\n",
    "        file_list = []\n",
    "        all_files = glob.glob(os.path.join(npy_dir, \"*.npy\"))\n",
    "        \n",
    "        print(f\"Scanning files for {len(target_ids)} IDs in {npy_dir}...\")\n",
    "        for fpath in all_files:\n",
    "            fname = os.path.basename(fpath)\n",
    "            file_id = fname.split('-')[0]\n",
    "            if file_id in target_ids:\n",
    "                file_list.append((fpath, file_id))\n",
    "        \n",
    "        print(f\"Found {len(file_list)} files. Loading into RAM...\")\n",
    "\n",
    "        # メモリへの一括読み込み\n",
    "        for fpath, sample_id in tqdm(file_list, desc=\"Pre-loading Data\"):\n",
    "            processed_data = self.process_one_file(fpath, sample_id, csv_dir)\n",
    "            if processed_data is not None:\n",
    "                self.samples.append(processed_data)\n",
    "                \n",
    "        print(f\"Successfully loaded {len(self.samples)} samples into RAM.\")\n",
    "\n",
    "    def process_one_file(self, npy_path, sample_id, csv_dir):\n",
    "        try:\n",
    "            # Input Loading\n",
    "            data = np.load(npy_path)\n",
    "            data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            original_len = data.shape[1]\n",
    "            if data.ndim != 2 or data.shape[0] != 13:\n",
    "                 return None\n",
    "\n",
    "            # Reconstruct\n",
    "            reconstructed = np.zeros((12, original_len), dtype=np.float32)\n",
    "            for i in range(4):\n",
    "                sig_row = data[i]\n",
    "                id_row = data[9+i]\n",
    "                unique_ids = np.unique(id_row)\n",
    "                for uid in unique_ids:\n",
    "                    if 0 <= uid <= 11:\n",
    "                        mask_ch = (id_row == uid)\n",
    "                        reconstructed[int(uid), mask_ch] = sig_row[mask_ch]\n",
    "            \n",
    "            # Target Loading\n",
    "            csv_path = os.path.join(csv_dir, f\"{sample_id}.csv\")\n",
    "            if not os.path.exists(csv_path):\n",
    "                return None\n",
    "\n",
    "            target_df = pd.read_csv(csv_path)\n",
    "            target_vals = target_df.values.T \n",
    "\n",
    "            # Mask Creation\n",
    "            mask_data = (~np.isnan(target_vals)).astype(np.float32)\n",
    "            target_data = np.nan_to_num(target_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # Resampling\n",
    "            if reconstructed.shape[1] != self.target_len:\n",
    "                input_final = resample(reconstructed, self.target_len, axis=1)\n",
    "            else:\n",
    "                input_final = reconstructed\n",
    "                \n",
    "            if target_data.shape[1] != self.target_len:\n",
    "                target_final = resample(target_data, self.target_len, axis=1)\n",
    "                mask_final = resample(mask_data, self.target_len, axis=1)\n",
    "            else:\n",
    "                target_final = target_data\n",
    "                mask_final = mask_data\n",
    "            \n",
    "            # Cleanup\n",
    "            input_final = np.nan_to_num(input_final, nan=0.0).astype(np.float32)\n",
    "            target_final = np.nan_to_num(target_final, nan=0.0).astype(np.float32)\n",
    "            mask_final = (np.nan_to_num(mask_final, nan=0.0) > 0.5).astype(np.float32)\n",
    "            \n",
    "            return (input_final, target_final, mask_final, original_len)\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_arr, target_arr, mask_arr, orig_len = self.samples[idx]\n",
    "        return (torch.from_numpy(input_arr), \n",
    "                torch.from_numpy(target_arr), \n",
    "                torch.from_numpy(mask_arr), \n",
    "                torch.tensor(orig_len, dtype=torch.long))\n",
    "\n",
    "# =========================================================\n",
    "# 2. Model (処理ロジック変更なし)\n",
    "# =========================================================\n",
    "class ResNet1d_UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(nn.Conv1d(12, 64, 7, 2, 3), nn.BatchNorm1d(64), nn.ReLU())\n",
    "        self.enc2 = nn.Sequential(nn.Conv1d(64, 128, 3, 2, 1), nn.BatchNorm1d(128), nn.ReLU())\n",
    "        self.enc3 = nn.Sequential(nn.Conv1d(128, 256, 3, 2, 1), nn.BatchNorm1d(256), nn.ReLU())\n",
    "        self.enc4 = nn.Sequential(nn.Conv1d(256, 512, 3, 2, 1), nn.BatchNorm1d(512), nn.ReLU())\n",
    "        \n",
    "        self.dec4 = nn.Sequential(nn.Conv1d(512 + 256, 256, 3, 1, 1), nn.BatchNorm1d(256), nn.ReLU())\n",
    "        self.dec3 = nn.Sequential(nn.Conv1d(256 + 128, 128, 3, 1, 1), nn.BatchNorm1d(128), nn.ReLU())\n",
    "        self.dec2 = nn.Sequential(nn.Conv1d(128 + 64, 64, 3, 1, 1), nn.BatchNorm1d(64), nn.ReLU())\n",
    "        self.dec1 = nn.Sequential(nn.Conv1d(64, 32, 3, 1, 1), nn.BatchNorm1d(32), nn.ReLU())\n",
    "        self.final = nn.Conv1d(32, 12, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        d4 = torch.cat([torch.nn.functional.interpolate(e4, size=e3.shape[2]), e3], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = torch.cat([torch.nn.functional.interpolate(d4, size=e2.shape[2]), e2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = torch.cat([torch.nn.functional.interpolate(d3, size=e1.shape[2]), e1], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = torch.nn.functional.interpolate(d2, size=x.shape[2])\n",
    "        d1 = self.dec1(d1)\n",
    "        out = self.final(d1)\n",
    "        return out\n",
    "\n",
    "# =========================================================\n",
    "# 3. Training Loop (ローカル対応版)\n",
    "# =========================================================\n",
    "def run_training():\n",
    "    # デバイス自動判定\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "    \n",
    "    # フォルダ作成\n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.makedirs(SAVE_DIR)\n",
    "    \n",
    "    if not os.path.exists(TRAIN_META):\n",
    "        raise FileNotFoundError(f\"Metadata file not found: {TRAIN_META}\")\n",
    "\n",
    "    df = pd.read_csv(TRAIN_META)\n",
    "    unique_ids = df['id'].unique()\n",
    "    train_ids, val_ids = train_test_split(unique_ids, test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_df = df[df['id'].isin(train_ids)].reset_index(drop=True)\n",
    "    val_df = df[df['id'].isin(val_ids)].reset_index(drop=True)\n",
    "    \n",
    "    print(\"Initializing Training Dataset...\")\n",
    "    train_dataset = ECGDatasetRam(train_df, npy_dir=STAGE2_DIR, csv_dir=CSV_DIR)\n",
    "    print(\"Initializing Validation Dataset...\")\n",
    "    val_dataset = ECGDatasetRam(val_df, npy_dir=STAGE2_DIR, csv_dir=CSV_DIR)\n",
    "    \n",
    "    # ローカル(Windows)では num_workers=0 が安定します (RAMロード済みなので速度差はほぼありません)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    model = ResNet1d_UNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7)\n",
    "    \n",
    "    # AMP設定 (GPUがある場合のみ有効化)\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "    \n",
    "    criterion_raw = nn.MSELoss(reduction='none')\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Training Started...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        current_epoch = epoch + 1\n",
    "        print(f\"\\n{'='*20} Epoch {current_epoch}/{EPOCHS} {'='*20}\")\n",
    "        \n",
    "        # --- Train ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Training   (Epoch {current_epoch})\")\n",
    "        \n",
    "        for inputs, targets, masks, _ in train_pbar:\n",
    "            inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # GPUならAMP使用、CPUなら通常計算\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = model(inputs)\n",
    "                    raw_loss = criterion_raw(outputs, targets)\n",
    "                    masked_loss = raw_loss * masks \n",
    "                    loss = masked_loss.sum() / (masks.sum() + 1e-8)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                raw_loss = criterion_raw(outputs, targets)\n",
    "                masked_loss = raw_loss * masks \n",
    "                loss = masked_loss.sum() / (masks.sum() + 1e-8)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f\"{loss.item():.6f}\"})\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # --- Valid ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        debug_batch_data = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, targets, masks, orig_lens) in enumerate(val_loader):\n",
    "                inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast('cuda'):\n",
    "                        outputs = model(inputs)\n",
    "                        raw_loss = criterion_raw(outputs, targets)\n",
    "                        masked_loss = raw_loss * masks\n",
    "                        loss = masked_loss.sum() / (masks.sum() + 1e-8)\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                    raw_loss = criterion_raw(outputs, targets)\n",
    "                    masked_loss = raw_loss * masks\n",
    "                    loss = masked_loss.sum() / (masks.sum() + 1e-8)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                if i == 0:\n",
    "                    debug_batch_data = {\n",
    "                        'input': inputs.cpu().float().numpy(),\n",
    "                        'target': targets.cpu().float().numpy(),\n",
    "                        'output': outputs.cpu().float().numpy(),\n",
    "                        'mask': masks.cpu().float().numpy(),\n",
    "                        'length': orig_lens.numpy()\n",
    "                    }\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Result: [Epoch {current_epoch}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | LR: {current_lr:.1e}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            print(f\"score improved: {best_loss:.6f} --> {avg_val_loss:.6f}\")\n",
    "            best_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # モデル保存\n",
    "            save_path = os.path.join(SAVE_DIR, \"best_resnet1d_unet.pth\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "            if debug_batch_data:\n",
    "                debug_path = os.path.join(SAVE_DIR, \"best_validation_debug.npz\")\n",
    "                np.savez(debug_path, **debug_batch_data)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "                \n",
    "    print(\"All Finished!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
